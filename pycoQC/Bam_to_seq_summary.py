#~~~~~~~~~~~~~~IMPORTS~~~~~~~~~~~~~~#
# Standard library imports
import os
from concurrent.futures import ThreadPoolExecutor

# Third party imports
from tqdm import tqdm
from dateutil.parser import parse
import pysam

# Local imports
from pycoQC.common import *
import functions


# Logger setup
logging.basicConfig(level=logging.INFO, format='%(message)s')
logger = logging.getLogger(__name__)
logLevel_dict = {2: logging.DEBUG, 1: logging.INFO, 0: logging.WARNING}


#~~~~~~~~~~~~~~CLASS~~~~~~~~~~~~~~#
class Bam_to_seq_summary():
    """
    Create a summary file akin the one generated by Albacore or Guppy from a directory containing
    multiple bam files. The script will attempt to extract all the required fields but will not
    raise an error if not found.
    """
    
    def __init__(self,
                 bam_dir: str,
                 seq_summary_fn: str,
                 max_bam: int = 0,
                 threads: int = 4,
                 basecall_id: int = 0,
                 verbose_level: int = 0,
                 include_path: bool = False,
                 fields: list = ("read_id", "run_id", "channel", "start_time", "gc_percent",
                                 "sequence_length_template", "mean_qscore_template",
                                 "calibration_strand_genome_template", "barcode_arrangement")):
        """
        * bam_dir
            Directory containing bamfiles. Can contain multiple subdirectories
        * seq_summary_fn
            path of the summary sequencing file where to write the data extracted from the bam files
        * max_bam
            Maximum number of file to try to parse. 0 to deactivate
        * threads
            Total number of threads to use. 1 thread is used for the reader and 1 for the writer. Minimum 3(default = 4)
        * fields
            list of field names corresponding to attributes to try to fetch from the bam files. List a valid field names:
            mean_qscore_template, sequence_length_template, called_events, skip_prob, stay_prob, step_prob, strand_score, read_id, start_time,
            duration, start_mux, read_number, channel, channel_digitisation, channel_offset, channel_range, channel_sampling,
            run_id, sample_id, device_id, protocol_run, flow_cell, calibration_strand, calibration_strand, calibration_strand,
            calibration_strand, barcode_arrangement, barcode_full, barcode_score, gc_percent
        * basecall_id
            id of the basecalling group. By default, leave to 0, but if you perform multiple basecalling on the same bam files,
            this can be used to indicate the corresponding group(1, 2 ...)
        * include_path
            If True the absolute path to the corresponding file is added in an extra column
        * verbose_level
            Level of verbosity, from 2(Chatty) to 0(Nothing)
        """
        # Set logging level
        logger.setLevel(logLevel_dict.get(verbose_level, logging.WARNING))

        # Perform checks
        logger.info("Check input data and options")
        if not os.access(bam_dir, os.R_OK):
            raise pycoQCError("Cannot read the indicated bam directory")
        if not os.access(os.path.dirname(seq_summary_fn), os.W_OK):
            raise pycoQCError("Cannot write the indicated seq_summary_fn")
        if threads < 3:
            raise pycoQCError("At least 3 threads required")

        # Save self args
        self.bam_dir = bam_dir
        self.seq_summary_fn = seq_summary_fn
        self.threads = threads - 2
        self.max_bam = max_bam
        self.fields = fields
        self.basecall_id = basecall_id
        self.include_path = include_path
        self.verbose_level = verbose_level

        # List bam files in folder
        bam_list = self.list_bam(bam_dir, self.max_bam)
        print('Found {} bam files to process'.format(len(bam_list)))

        # Fetch relevant info from bam files in parallel
        master_bam_dict = dict()
        with ThreadPoolExecutor(max_workers=self.threads) as executor:
            for results in list(tqdm(executor.map(Bam_to_seq_summary.read_bam, bam_list),
                                     total=len(bam_list))):
                master_bam_dict.update(results)

        print('Writing "sequencing_summary" file...')
        Bam_to_seq_summary.write_seq_summary(master_bam_dict, self.seq_summary_fn)

        print('Done')

    @staticmethod
    def list_bam(bam_dir, max_bam):
        """
        Mono-threaded worker adding bam files found in a directory tree recursively
        to a feeder queue for the multiprocessing workers
        """
        logger.debug("[READER] Start listing bam files")
        bam_list = list()

        # Load an input queue with bam file path
        for i, bam_file in enumerate(recursive_file_gen(dir=bam_dir, ext='bam')):
            if max_bam and i == max_bam:
                break
            bam_list.append(bam_file)

        # Raise error is no file found
        if not bam_list:
            raise pycoQCError("No valid bam files found in indicated folder")

        logger.debug("[READER] Add a total of {} files to input queue".format(len(bam_list)))

        return bam_list

    @staticmethod
    def read_bam(bam_file):

        logger.debug("[WORKER] Start processing pod5 files")

        entry_dict = dict()

        with pysam.AlignmentFile(bam_file, "rb", check_sq=False) as fh:
            for bam_entry in fh.fetch(until_eof=True):
                bam_dict = dict()
                read_id = bam_entry.query_name
                header = str(bam_entry.header)
                for line in header.split("\n"):
                    if line.startswith("@RG"):
                        field_list = line.split("\t")
                        run_id = [i for i in field_list if i.startswith('ID:')][0].split('_')[0].split(':')[1]
                        flowcell = [i for i in field_list if i.startswith('PU:')][0].split(':')[1]
                        sequencer = [i for i in field_list if i.startswith('PM:')][0].split(':')[1]
                        model = [i for i in field_list if i.startswith('DS:')][0].split()[0].split('=')[1]

                tags = bam_entry.tags  # List of tuples
                """
                BC:
                qs:i:10  # Q-score
                du:f:31.6362  # Duration of the read (in seconds)
                ns:i:126545  # Number of signals prior trimming
                ts:i:170  # Number of signals trimmed from start
                mx:i:1  # read mux
                ch:i:5  # Channel
                st:Z:2021-03-29T15:18:10.066+00:00  # Start time
                rn:i:6347  # read number
                fn:FAW88668_pass_barcode06_1ec22c6e_7ac59ce2_5.pod5  # File name
                sm:f:98.1631  # scaling midpoint/mean/median (pA to ~0-mean/1-sd)
                sd:f:13.5198  # scaling dispersion (pA to ~0-mean/1-sd)
                sv:Z:med_mad  # scaling version
                dx:i:0  # 
                RG:Z:e2419f1214664b2b48a1f01eebf8dfa6f28fb57a_dna_r9.4.1_e8_sup@v3.3  # <runid>_<basecalling_model>
                """
                tags_of_interest = ['BC', 'qs', 'du', 'ch', 'st', 'rn', 'fn']
                for tag_tuple in tags:
                    tag, value = tag_tuple
                    if tag in tags_of_interest:
                        bam_dict[tag] = value
                seq = bam_entry.seq
                length = bam_entry.query_length
                qual = bam_entry.qual

                time_string = parse(bam_dict['st'])
                channel = bam_dict['ch']
                try:
                    barcode = bam_dict['BC']
                except KeyError:
                    barcode = 'none'
                file_name = bam_dict['fn']

                # Flag
                if 'pass' in file_name:
                    flag = 'TRUE'
                elif 'fail' in file_name:
                    flag = 'FALSE'
                else:
                    flag = 'TRUE'

                # Average phred score
                phred_list = [ord(letter) - 33 for letter in qual]
                average_phred = round(functions.compute_average_quality(phred_list, length), 2)  # cython

                # %GC
                g_count = float(seq.count('G'))
                c_count = float(seq.count('C'))
                gc = round((g_count + c_count) / float(length) * 100, 2)

                # Output everything in a dictionary
                entry_dict[read_id] = dict()
                entry_dict[read_id]['run_id'] = run_id
                entry_dict[read_id]['channel'] = channel
                entry_dict[read_id]['barcode_arrangement'] = barcode
                entry_dict[read_id]['sequence_length_template'] = length
                entry_dict[read_id]['mean_qscore_template'] = average_phred
                entry_dict[read_id]['gc_percent'] = gc
                entry_dict[read_id]['start_time'] = time_string
                entry_dict[read_id]['passes_filtering'] = flag
                entry_dict[read_id]['flowcell_id'] = flowcell
                entry_dict[read_id]['device_id'] = sequencer
                entry_dict[read_id]['dorado_model'] = model
                entry_dict[read_id]['filename'] = file_name

        return entry_dict

    @staticmethod
    def write_seq_summary(bam_dict, output_file):
        """
        Mono-threaded Worker writing the sequencing summary file
        """
        logger.debug("[WRITER] Write data to file")

        # Convert dictionary to dataframe
        df = pd.DataFrame.from_dict(bam_dict, orient='index')
        df.index.names = ['read_id']

        # Convert "start_time" column to datetime
        df['start_time'] = pd.to_datetime(df['start_time'])

        # Convert datetime to elapsed minutes
        time_zero = df.loc[:, 'start_time'].min()  # looking for min of 1st elements of list of tuples
        df.loc[:, 'start_time'] = df.loc[:, 'start_time'] - time_zero
        df.loc[:, 'start_time'] = df.loc[:, 'start_time'].dt.total_seconds()
        try:  #
            df.loc[:, 'start_time'] = df.loc[:, 'start_time'].astype(int)  # Convert to integer
        except pd.errors.IntCastingNaNError:
            pass

        df.to_csv(output_file, sep="\t", index=True, header=True)
